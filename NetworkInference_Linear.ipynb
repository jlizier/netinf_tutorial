{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f1115d-ff50-4db2-8ed3-f8ccf408ae15",
   "metadata": {},
   "source": [
    "# Directed functional/effective network inference with linear methods\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This notebook requires the following python libraries to be installed for your environment:\n",
    "1. The usual suspects: `numpy`, `scipy`, `matplotlib`, `os`\n",
    "2. The `stocks` data set requires `yfinance` and `pandas`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this activity, we use linear methods to infer functional and effective connectivity, choosing between the following data sets:\n",
    "1. Synthetic VAR dynamics on a Watts-Strogatz small world network\n",
    "2. Stock market closing prices from the NYSE\n",
    "3. fMRI recording of a movie watching task\n",
    "\n",
    "Select which data set to use via the `dataSet` variable below.\n",
    "Then, running the following code cell will read in or generate the data and make a carpet plot of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd0c1a-b779-449e-804f-d1c02e24dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mplcolors\n",
    "import os\n",
    "\n",
    "# Data set choices: 'syntheticVAR', 'fmri_movie', 'stocks' (and maybe 'mouse')\n",
    "dataSet = 'syntheticVAR';\n",
    "\n",
    "# Processing choices:\n",
    "normalise = True\n",
    "clusterFC = 'auto' # can be True, False or auto\n",
    "labelfontsize = 4\n",
    "\n",
    "# Load/prepare the data:\n",
    "if (dataSet == 'syntheticVAR'):\n",
    "    # Generate some VAR data run on a Watts-Strogatz ring network with d=4, p=0.1, self-weights=0.3, cross-weights=0.125\n",
    "    matContents = scipy.io.loadmat('data/ws_ring_C.mat')\n",
    "    C = np.array(matContents['C']); # C[i,j] is the connection from i to j, so we use row vectors here\n",
    "    plt.figure()\n",
    "    plt.imshow(C, aspect='auto')\n",
    "    plt.xlabel('target');\n",
    "    plt.ylabel('source');\n",
    "    plt.title('Directed structural matrix');\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Weight');\n",
    "    \n",
    "    # Generate some time series data on this network:\n",
    "    N = 100  # Number of variables\n",
    "    S = 1000   # Number of time-series samples -- you can add more and watch accuracy increase\n",
    "    initialTimeSteps = 10  # Warm-up steps\n",
    "    # Initialize row vector X with standard normal values\n",
    "    X = np.random.normal(0, 1, (1, N))\n",
    "    # Initialize matrix to store samples\n",
    "    data = np.zeros((S, N))\n",
    "    sampleNum = 0\n",
    "    for t in range(initialTimeSteps + S):\n",
    "        newX = X @ C + np.random.normal(0, 1, (1, N))\n",
    "        if t >= initialTimeSteps:\n",
    "            data[sampleNum, :] = newX\n",
    "            sampleNum += 1\n",
    "        X = newX\n",
    "    labels = np.arange(data.shape[1])\n",
    "    if (clusterFC == 'auto'):\n",
    "        clusterFC = False;\n",
    "    dataMethod = 'Generated';\n",
    "elif (dataSet == 'stocks'):\n",
    "    stocksFile = 'data/stockClosingPricesData.npy';\n",
    "    labelsFile = 'data/stockClosingPricesLabels.npy';\n",
    "    if (os.path.exists(stocksFile)):\n",
    "        print('Found stocks file');\n",
    "        data = np.load(stocksFile);\n",
    "        labels = np.load(labelsFile, allow_pickle=True);\n",
    "    else:\n",
    "        print('Downloading NYSE stock closing prices');\n",
    "        import yfinance as yf\n",
    "        import pandas as pd\n",
    "        # Define list of NYSE ticker symbols\n",
    "        with open('data/SandP100.txt', 'r') as file:\n",
    "            tickers = file.read().splitlines()\n",
    "        # Download historical data from 2000 to today\n",
    "        fullData = yf.download(tickers, start=\"2000-01-01\", auto_adjust=True)\n",
    "        # Keep only stocks with data available on all trading days:\n",
    "        fullData = fullData.dropna(axis='columns')\n",
    "        close_prices = fullData['Close']\n",
    "        # data = close_prices.dropna(axis=1, how='any')\n",
    "        # Alternatively, fill the nans with previous day's price:\n",
    "        # data = close_prices.ffill()\n",
    "        data = close_prices # All bad data already removed\n",
    "        # Following doesn't work, fix later\n",
    "        # tickers = fullData['Tickers'] # Grab tickers for the final data only\n",
    "        data = np.array(data)\n",
    "        # print('Before log-differences %d samples for %d variables' % (data.shape[0], data.shape[1]))\n",
    "        # And take log differences:\n",
    "        data = np.log(data[1:,:]) - np.log(data[:-1,:])\n",
    "        labels = np.array(fullData.Close.keys());\n",
    "        np.save(stocksFile, data)\n",
    "        np.save(labelsFile, labels)\n",
    "    if (clusterFC == 'auto'):\n",
    "        clusterFC = True;\n",
    "    dataMethod = 'Read in';\n",
    "elif (dataSet == 'fmri_movie'):\n",
    "    # Downloaded from nilearn's fetch_atlas_msdl dataset\n",
    "    fmriTimeSeriesFile = 'data/development_fmri_msdl_subject1.npy';\n",
    "    labelsFile = 'data/development_fmri_msdl_roiLabels.npy';\n",
    "    if (os.path.exists(fmriTimeSeriesFile)):\n",
    "        print('Found fMRI movie watching file');\n",
    "        data = np.load(fmriTimeSeriesFile);\n",
    "        labels = np.load(labelsFile);\n",
    "    else:\n",
    "        from nilearn import datasets\n",
    "        from nilearn.maskers import NiftiMapsMasker\n",
    "        atlas = datasets.fetch_atlas_msdl()\n",
    "        # Loading atlas image stored in 'maps'\n",
    "        atlas_filename = atlas[\"maps\"]\n",
    "        # Loading atlas data stored in 'labels'\n",
    "        labels = atlas[\"labels\"]\n",
    "        # Loading the functional datasets\n",
    "        dataImages = datasets.fetch_development_fmri(n_subjects=1)\n",
    "        # print basic information on the dataset\n",
    "        print(f\"First subject functional nifti images (4D) are at: {dataImages.func[0]}\")\n",
    "        masker = NiftiMapsMasker(\n",
    "            maps_img=atlas_filename,\n",
    "            standardize=\"zscore_sample\",\n",
    "            standardize_confounds=\"zscore_sample\",\n",
    "            memory=\"nilearn_cache\",\n",
    "            verbose=5,\n",
    "        )\n",
    "        data = masker.fit_transform(dataImages.func[0], confounds=dataImages.confounds)\n",
    "        import numpy as np\n",
    "        np.save(fmriTimeSeriesFile, data)\n",
    "        np.save(labelsFile, labels)\n",
    "    if (clusterFC == 'auto'):\n",
    "        clusterFC = False;\n",
    "    labelfontsize = 6;\n",
    "    dataMethod = 'Read in';\n",
    "# elif (dataSet == 'mouse'):\n",
    "#    # TODO: need permission to distribute preprocessed data here or add code to process:\n",
    "#    matContents = scipy.io.loadmat('data/18MicefMRISubset.mat')\n",
    "#    # As numpy array:\n",
    "#    timeSeries = np.array(matContents['timeSeries']);\n",
    "#    data = timeSeries[:,:,1].T # We want time as first index, variable as second\n",
    "#    labels = np.arange(data.shape[1]) # Add these later\n",
    "#    if (clusterFC == 'auto'):\n",
    "#        clusterFC = False;\n",
    "#    dataMethod = 'Read in';\n",
    "else:\n",
    "    raise SystemExit('Invalid dataSet %s' % dataSet)\n",
    "\n",
    "timeSteps = data.shape[0]\n",
    "networkSize = data.shape[1]\n",
    "print('%s data with %d time steps for %d variables' % (dataMethod, data.shape[0], data.shape[1]))\n",
    "\n",
    "# Normalise the data?\n",
    "if (normalise):\n",
    "    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "# Check: print(np.mean(data, axis=0))\n",
    "if (clusterFC):\n",
    "    clusterSuffix = ' clustered'\n",
    "else:\n",
    "    clusterSuffix = ''\n",
    "\n",
    "# Make a carpet plot of the raw data:\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(data.T, aspect='auto')\n",
    "plt.xlabel('time');\n",
    "plt.ylabel('variable');\n",
    "plt.yticks(ticks=np.arange(len(labels)), labels=labels, fontsize=labelfontsize)\n",
    "plt.title('Raw data');\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Activity');\n",
    "if (dataSet == 'stocks'):\n",
    "    plt.clim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159f877-cac5-40e7-99f7-12f250e0c558",
   "metadata": {},
   "source": [
    "## A. Compute functional connectivity\n",
    "\n",
    "First we will infer the functional network to represent the pairwise statistical relationships between the variables as the nodes in our network. We'll use correlation for these discrete valued variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecd9cd-8259-49e8-8a85-d6ba78286696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = np.zeros((networkSize,networkSize));\n",
    "pValues = np.zeros((networkSize,networkSize));\n",
    "\n",
    "# Here in principle we could use corrcoef(data, rowvar=False) however this does not provide statistical significance calculations.\n",
    "# So instead we loop over calls to scipy.pearsonr:\n",
    "for s in range(networkSize):\n",
    "    for t in range(networkSize):\n",
    "        # For each source-dest pair:\n",
    "        if (s == t):\n",
    "            continue\n",
    "        # We could compute only once for each pair, but leave the loop in case we do an undirected measure later:\n",
    "        fc[s,t], pValues[s,t] = stats.pearsonr(data[:,s],data[:,t])\n",
    "\n",
    "if (clusterFC):\n",
    "    from scipy.cluster.hierarchy import linkage, optimal_leaf_ordering, leaves_list\n",
    "    from scipy.spatial.distance import pdist\n",
    "    # Perform hierarchical clustering using average linkage\n",
    "    distances = np.max(fc) - fc\n",
    "    Z = linkage(distances, method='average')\n",
    "    # Compute the optimal leaf ordering\n",
    "    optimal_Z = optimal_leaf_ordering(Z, distances)\n",
    "    # Get the order of the leaves\n",
    "    optimal_order = leaves_list(optimal_Z)\n",
    "else:\n",
    "    optimal_order = np.arange(networkSize); # won't change anything\n",
    "\n",
    "# Plot the FC:\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1) # left subplot\n",
    "plt.imshow(fc[optimal_order,:][:,optimal_order],cmap=cm.coolwarm, norm=mplcolors.CenteredNorm())\n",
    "plt.xlabel('target');\n",
    "plt.xticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], rotation=90, fontsize=labelfontsize)\n",
    "plt.ylabel('source');\n",
    "plt.yticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], fontsize=labelfontsize)\n",
    "plt.title('FC between all pairs%s' % clusterSuffix);\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('FC');\n",
    "plt.subplot(1,2,2) # right subplot\n",
    "plt.imshow(pValues[optimal_order, :][:, optimal_order])\n",
    "plt.xlabel('target');\n",
    "plt.xticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], rotation=90, fontsize=labelfontsize)\n",
    "plt.ylabel('source');\n",
    "plt.yticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], fontsize=labelfontsize)\n",
    "plt.title('p-values for FC%s' % clusterSuffix);\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('p-values');\n",
    "plt.clim(0,0.05/(networkSize*(networkSize-1))); # Can use this to highlight significant p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61a350-c600-4b80-8dfa-ecab096ad392",
   "metadata": {},
   "source": [
    "Look at the results for the adjacency matrix for the functional network on the left figure above:\n",
    "* What do you observe in terms of the functional network structure? Is there stucture as to which sources have the highest MI to each target? (you could try setting `clusterFC=True` in the first code block if anhy clustering is not clear.\n",
    "* For the other data sets, are the strongly related nodes sensible pairings (in terms of neural / financial knowledge)?\n",
    "* If using the synthetic data set, how does it compare to what you know the underlying causal structure to be? Is that what you expected to see?\n",
    "\n",
    "_Challenge_: try to correct for the autocorrelations in these samples using Oliver Cliff's [assessing-linear-dependence](https://github.com/olivercliff/assessing-linear-dependence) code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ab3ea-b4aa-41a1-bc06-b6072d28b088",
   "metadata": {},
   "source": [
    "## B. Directed functional network inference with Transfer Entropy (TE)\n",
    "\n",
    "Next we compute directed pairwise statistical relationships in the network via the lagged covariance matrix (it remains a lagged correlation, or close enough if you left `normalise = True` in the first code block). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a5edb-6765-4c55-aa95-9b0155b30946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contemporaneous covariance (compute now since we'll need it later anyway)\n",
    "cov = np.cov(data, rowvar=False) # Will match the correlation above given we've normalised, except for the diagonal\n",
    "# Lag 1 covariance, between time step before and time step after:\n",
    "cov_lag1 = np.dot(data[:-1,:].T, data[1:,:]) / (timeSteps - 1)\n",
    "\n",
    "plt.imshow(cov_lag1[optimal_order, :][:, optimal_order],cmap=cm.coolwarm, norm=mplcolors.CenteredNorm())\n",
    "plt.xlabel('target');\n",
    "plt.xticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], rotation=90, fontsize=labelfontsize)\n",
    "plt.ylabel('source');\n",
    "plt.yticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], fontsize=labelfontsize)\n",
    "plt.title('Lagged correlation between all pairs%s' % clusterSuffix);\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Lagged covariance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b8bd8-c9cf-4110-834f-eb2eda31ec4a",
   "metadata": {},
   "source": [
    "_Challenge_: adapt the code for lagged correlation to return p-values for each pair (you'll need to replace the single line calculation with loops like for correlation), and plot these like for correlation.\n",
    "_Further challenge_: use the [assessing-linear-dependence](https://github.com/olivercliff/assessing-linear-dependence) code as above to control for autocorrelations here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec095a5b-6c4d-41b4-a9af-f56ea1613f3d",
   "metadata": {},
   "source": [
    "## C. Effective network inference with least squares regression\n",
    "\n",
    "Finally, we'll construct effective connectivity via a least squares regression applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8117ba9-dc97-4d59-994f-40cb9372decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now invert: cov_lag1 = cov * C in our notation, so\n",
    "# C = cov^-1 * cov_lag1\n",
    "inferredC = np.linalg.inv(cov) @ cov_lag1;\n",
    "\n",
    "if (dataSet == 'synthetic'):\n",
    "    # We have a ground truth so we'll plot that as well. Make preparations here:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1) # left subplot\n",
    "plt.imshow(inferredC[optimal_order, :][:, optimal_order], cmap=cm.coolwarm, norm=mplcolors.CenteredNorm())\n",
    "plt.xlabel('target');\n",
    "plt.xticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], rotation=90, fontsize=4)\n",
    "plt.ylabel('source');\n",
    "plt.yticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], fontsize=4)\n",
    "plt.title('Least squares EC between all pairs%s' % clusterSuffix);\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Weight');\n",
    "if (dataSet == 'synthetic'):\n",
    "    # We have a ground truth so we'll plot that as well:\n",
    "    plt.subplot(1,2,2) # right subplot\n",
    "    plt.imshow(C[optimal_order, :][:, optimal_order])\n",
    "    plt.xlabel('target');\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], rotation=90, fontsize=labelfontsize)\n",
    "    plt.ylabel('source');\n",
    "    plt.yticks(ticks=np.arange(len(labels)), labels=labels[optimal_order], fontsize=labelfontsize)\n",
    "    plt.title('Directed structural matrix%s' % clusterSuffix);\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac3c1e-c635-44b3-9cb2-e180583106fd",
   "metadata": {},
   "source": [
    "Reflect:\n",
    "* How do the results compare to your expectation?\n",
    "* If you're using the synthetic network, how might you separate what is a true connection from a spurious one here? Do the results improve as we use more time samples (you can change this in the first code block)?\n",
    "* For the other data sets, how might we improve our results (e.g. more time samples, more subjects to combine across, other methods ...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67555a-d7f3-4e63-921f-eaefbc7cf323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
